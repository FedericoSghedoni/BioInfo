0. Running command for hyperparameters (1e-05, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 1e-05 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logz_dropout
1. Running command for hyperparameters (1e-05, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 1e-05 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logz_dropout

2. Running command for hyperparameters (1e-05, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume True --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 1e-05 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logz_dropout
2. Running command for hyperparameters (1e-05, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 1e-05 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logz_dropout
3. Running command for hyperparameters (1e-05, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 1e-05 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logz_dropout
