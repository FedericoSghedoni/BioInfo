0. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
1. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logy_dropout
2. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
2. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume True --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
3. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
4. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logy_dropout
5. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
5. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume True --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
6. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume True --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
6. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
7. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logy_dropout
8. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
0. Running command for hyperparameters (0.01, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
9. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
10. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logy_dropout
10. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logy_dropout
11. Running command for hyperparameters (0.01, 'num_warmup_steps=5415', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
12. Running command for hyperparameters (0.01, 'num_warmup_steps=13415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=13415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
12. Running command for hyperparameters (0.01, 'num_warmup_steps=13415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=13415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
13. Running command for hyperparameters (0.01, 'num_warmup_steps=13415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=13415 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logy_dropout
14. Running command for hyperparameters (0.01, 'num_warmup_steps=13415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=13415 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logy_dropout
15. Running command for hyperparameters (0.01, 'num_warmup_steps=13415', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.01 --scheduler_kwargs num_warmup_steps=13415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logy_dropout
