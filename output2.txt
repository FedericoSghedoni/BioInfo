0. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logw_dropout
1. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logw_dropout
2. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logw_dropout
3. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logw_dropout
3. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume True --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logw_dropout
4. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logw_dropout
5. Running command for hyperparameters (0.001, 'num_warmup_steps=10000', 0.0001, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.0001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0001 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=10000 --model_kwargs pretrained=True, hidden_dropout_prob=0.0001 --log_dir ./logw_dropout
6. Running command for hyperparameters (0.001, 'num_warmup_steps=5415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.01'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume True --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.01 --log_dir ./logz_dropout
7. Running command for hyperparameters (0.001, 'num_warmup_steps=5415', 0.0002, 'cutmix2', 'pretrained=True, hidden_dropout_prob=0.001'): python WildsDataset/examples/run_expt.py --dataset rxrx1 --algorithm ERM --root_dir data --device 0 --resume False --n_epochs 20 --model google/vit-base-patch16-224 --additional_train_transform cutmix2 --batch_size 16 --lr 0.0002 --weight_decay 0.001 --scheduler_kwargs num_warmup_steps=5415 --model_kwargs pretrained=True, hidden_dropout_prob=0.001 --log_dir ./logz_dropout
